{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c4ac8c",
   "metadata": {},
   "source": [
    "# Foundations of NLP\n",
    "\n",
    "\n",
    "**Definitions:**\n",
    "- **Tokenization**: Splitting text into words, subwords, or characters.\n",
    "- **Stopword Removal**: Removing common words that add little meaning.\n",
    "- **Stemming**: Reducing words to their root form by chopping suffixes.\n",
    "- **Lemmatization**: Reducing words to dictionary form using grammar rules.\n",
    "- **POS Tagging**: Assigning grammatical categories to words.\n",
    "- **Dependency Parsing**: Finding grammatical relationships between words.\n",
    "- **NER**: Identifying named entities like people, locations, dates.\n",
    "- **BoW/TF-IDF**: Representing text as frequency-based vectors.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0be837c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T14:43:37.733069Z",
     "iopub.status.busy": "2025-10-01T14:43:37.730751Z",
     "iopub.status.idle": "2025-10-01T14:43:40.465848Z",
     "shell.execute_reply": "2025-10-01T14:43:40.464628Z",
     "shell.execute_reply.started": "2025-10-01T14:43:37.733038Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['OpenAI', 'creates', 'powerful', 'AI', 'models', 'like', 'GPT-4', '.']\n",
      "Stopwords removed: ['OpenAI', 'creates', 'powerful', 'AI', 'models', 'like', 'GPT-4', '.']\n",
      "Stems: ['openai', 'creat', 'power', 'ai', 'model', 'like', 'gpt-4', '.']\n",
      "Lemmas: ['OpenAI', 'creates', 'powerful', 'AI', 'model', 'like', 'GPT-4', '.']\n",
      "POS tags: [('OpenAI', 'NNP'), ('creates', 'VBZ'), ('powerful', 'JJ'), ('AI', 'NNP'), ('models', 'NNS'), ('like', 'IN'), ('GPT-4', 'NNP'), ('.', '.')]\n",
      "Named Entities: (S\n",
      "  (ORGANIZATION OpenAI/NNP)\n",
      "  creates/VBZ\n",
      "  powerful/JJ\n",
      "  AI/NNP\n",
      "  models/NNS\n",
      "  like/IN\n",
      "  GPT-4/NNP\n",
      "  ./.)\n",
      "BoW: [[1 1 1 1 1 1 1]]\n",
      "TF-IDF: [[0.37796447 0.37796447 0.37796447 0.37796447 0.37796447 0.37796447\n",
      "  0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"maxent_ne_chunker_tab\")\n",
    "nltk.download(\"words\")\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "\n",
    "text = \"OpenAI creates powerful AI models like GPT-4.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "filtered = [w for w in tokens if w.lower() not in stopwords.words(\"english\")]\n",
    "print(\"Stopwords removed:\", filtered)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(\"Stems:\", [stemmer.stem(w) for w in filtered])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Lemmas:\", [lemmatizer.lemmatize(w) for w in filtered])\n",
    "\n",
    "print(\"POS tags:\", pos_tag(tokens))\n",
    "print(\"Named Entities:\", ne_chunk(pos_tag(tokens)))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print(\"BoW:\", vectorizer.fit_transform([text]).toarray())\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "print(\"TF-IDF:\", tfidf.fit_transform([text]).toarray())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c103a3f",
   "metadata": {},
   "source": [
    "# Neural NLP\n",
    "\n",
    "\n",
    "**Definitions:**\n",
    "- **RNN/LSTM/GRU**: Neural networks for sequential data, handling dependencies.\n",
    "- **Attention**: Focus mechanism to weigh important parts of input.\n",
    "- **Transformers**: Models built on attention, parallelizable.\n",
    "- **Encoder/Decoder**: Encoder processes input; decoder generates output.\n",
    "- **Pretraining/Fine-tuning**: Training on large data first, then adapting to tasks.\n",
    "- **BERT**: Encoder-only model using Masked LM.\n",
    "- **GPT**: Decoder-only autoregressive model.\n",
    "- **T5/BART**: Seq2Seq transformer models.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4538cc00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T14:43:48.662422Z",
     "iopub.status.busy": "2025-10-01T14:43:48.661884Z",
     "iopub.status.idle": "2025-10-01T14:55:12.957717Z",
     "shell.execute_reply": "2025-10-01T14:55:12.951797Z",
     "shell.execute_reply.started": "2025-10-01T14:43:48.662390Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6386a1047c6148ada08b92982de8f9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067d6a581d6440fd8e7a51874f32ce63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98983b0805304c1f994015f70a48b694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfacc322cfc42288347275c04292f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998449087142944}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f04f4f84ad46a6a170f4f374cde25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4bfb1ef6f14d63b117e30b3632f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1744d6348add4a948895a60acf1386f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f201dc552a364427a7453515b7af4847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2fd55b419a480c8b73a311c1666a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5242969393730164}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e3d2627c3049de8d77e3eb3f3a7e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5c97a9ada648d6a742e9f39cc05ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce535714c56414dbb577eebdf2c5c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd55a39771f4853adf4878de84872fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': np.float32(0.99917614), 'word': 'Barack Obama', 'start': 0, 'end': 12}, {'entity_group': 'LOC', 'score': np.float32(0.99945), 'word': 'Hawaii', 'start': 25, 'end': 31}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410c807887cb4da981644f65a7e4ef02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f575f78a1be049b1a4ed362e93ce6021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0b251fec0f4366bcbcb478436bfba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e899c128be743ecbedcc831da8189ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02736ee2f98540a49bfae46008092646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7013d24e609c474ea2515fb067374003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'NLP est fascinant !'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment(\"I love NLP!\"))\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")\n",
    "print(classifier(\"The stock market is going up.\"))\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "print(ner(\"Barack Obama was born in Hawaii.\"))\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "print(translator(\"NLP is fascinating!\", max_length=40))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d7a43",
   "metadata": {},
   "source": [
    "# Generative AI Core Concepts\n",
    "\n",
    "\n",
    "**Definitions:**\n",
    "- **Autoregressive LM**: Predicts next token (GPT).\n",
    "- **Seq2Seq**: Input to output sequences (T5, BART).\n",
    "- **Generation Methods**: Greedy, Beam, Top-k, Top-p sampling.\n",
    "- **Instruction Tuning**: Training to follow instructions.\n",
    "- **RLHF**: Aligning models with human feedback.\n",
    "- **LoRA/PEFT**: Efficient fine-tuning.\n",
    "- **Evaluation Metrics**: Perplexity, BLEU, ROUGE.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaeaeeaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:00:47.817401Z",
     "iopub.status.busy": "2025-10-01T15:00:47.816853Z",
     "iopub.status.idle": "2025-10-01T15:01:16.769157Z",
     "shell.execute_reply": "2025-10-01T15:01:16.767464Z",
     "shell.execute_reply.started": "2025-10-01T15:00:47.817355Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam: Once upon a time, it was said, there would be a time when the world would be a better place.\n",
      "\n",
      "It was a time when\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k: Once upon a time, most of us grew up believing in God, believing that he was the final and perfect figure that needed to be given the throne\n",
      "Top-p: Once upon a time, the whole universe became one gigantic, multicellular being that couldn't even fly.\n",
      "\n",
      "\"The universe is all this\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\")\n",
    "\n",
    "greedy_output = model.generate(**inputs, max_length=30)\n",
    "print(\"Greedy:\", tokenizer.decode(greedy_output[0]))\n",
    "\n",
    "beam_output = model.generate(**inputs, max_length=30, num_beams=5, early_stopping=True)\n",
    "print(\"Beam:\", tokenizer.decode(beam_output[0]))\n",
    "\n",
    "topk_output = model.generate(**inputs, max_length=30, do_sample=True, top_k=50)\n",
    "print(\"Top-k:\", tokenizer.decode(topk_output[0]))\n",
    "\n",
    "topp_output = model.generate(**inputs, max_length=30, do_sample=True, top_p=0.9)\n",
    "print(\"Top-p:\", tokenizer.decode(topp_output[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae59fb",
   "metadata": {},
   "source": [
    "# Advanced NLP\n",
    "\n",
    "\n",
    "**Definitions:**\n",
    "- **Multimodal NLP**: Combining text with image/audio/video.\n",
    "- **CLIP**: Aligns text and image embeddings.\n",
    "- **RAG**: Combines retrieval with generation.\n",
    "- **Embeddings**: Dense vector representations for similarity search.\n",
    "- **Hybrid Search**: Combining keyword and dense search.\n",
    "- **Long-context Models**: Models designed for large inputs (Longformer).\n",
    "- **Hallucination Handling**: Mitigating incorrect generations.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c67c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:01:47.812189Z",
     "iopub.status.busy": "2025-10-01T15:01:47.811801Z",
     "iopub.status.idle": "2025-10-01T15:01:52.238952Z",
     "shell.execute_reply": "2025-10-01T15:01:52.237751Z",
     "shell.execute_reply.started": "2025-10-01T15:01:47.812169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: tensor([[0.6641]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sent1 = \"Artificial Intelligence is the future.\"\n",
    "sent2 = \"AI will change the world.\"\n",
    "emb1 = embedder.encode(sent1, convert_to_tensor=True)\n",
    "emb2 = embedder.encode(sent2, convert_to_tensor=True)\n",
    "print(\"Cosine similarity:\", util.pytorch_cos_sim(emb1, emb2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d14fca",
   "metadata": {},
   "source": [
    "# Cutting-edge Research\n",
    "\n",
    "\n",
    "**Definitions:**\n",
    "- **Zero/Few-shot Learning**: Performing tasks with no or few examples.\n",
    "- **Chain-of-Thought**: Prompting models to reason step by step.\n",
    "- **LangChain/LlamaIndex**: Frameworks for LLM apps and RAG.\n",
    "- **Quantization**: Reducing precision for efficiency.\n",
    "- **Distillation**: Training small model from large one.\n",
    "- **MoE**: Mixture of Experts architecture.\n",
    "- **Alignment**: Making models safe and human-aligned.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56caf9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:02:21.941671Z",
     "iopub.status.busy": "2025-10-01T15:02:21.941309Z",
     "iopub.status.idle": "2025-10-01T15:09:39.827279Z",
     "shell.execute_reply": "2025-10-01T15:09:39.822329Z",
     "shell.execute_reply.started": "2025-10-01T15:02:21.941649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13c61df9ce249999e43fbbb31b103e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  13%|#2        | 210M/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce82f4a586d3434e83aaec872f1a2f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c0d1fbbc4840d2b3b8059398785d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74333c47f91440bfbbf9ecbc5a9acf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48176d50164e40fbbff47a4158cfd731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'I love using AI for healthcare.', 'labels': ['health', 'education', 'finance'], 'scores': [0.9965513944625854, 0.0017774497391656041, 0.0016711429925635457]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot: Classify the sentiment:\n",
      "Text: \"The product is amazing\"\n",
      "Sentiment: Positive\n",
      "Text: \"This is the worst purchase ever\"\n",
      "Sentiment: Negative\n",
      "Text: \"I think it's okay, not great\"\n",
      "Sentiment: Positive\n",
      "Text: \"I'm not sure what to say\"\n",
      "Sentiment: Negative\n",
      "Text: \"I'm not sure what to say\"\n",
      "Sentiment: Negative\n",
      "Text: \"I'm not sure what to say\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "print(zero_shot(\"I love using AI for healthcare.\", candidate_labels=[\"education\", \"health\", \"finance\"]))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = '''Classify the sentiment:\n",
    "Text: \"The product is amazing\"\n",
    "Sentiment: Positive\n",
    "Text: \"This is the worst purchase ever\"\n",
    "Sentiment: Negative\n",
    "Text: \"I think it's okay, not great\"\n",
    "Sentiment:'''\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "print(\"Few-shot:\", tokenizer.decode(outputs[0]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
